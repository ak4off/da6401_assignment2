[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'finetune_partB_Assgn2' when running a sweep.
[34m[1mwandb[0m: [33mWARNING[0m Ignoring entity 'ns24z274-iitm-ac-in' when running a sweep.
Traceback (most recent call last):
  File "/home/speech/da6401/A2/mod_17a/GIT/da6401_assignment2/partB/the_sweep.py", line 19, in sweep_train
    train(args)
  File "/home/speech/da6401/A2/mod_17a/GIT/da6401_assignment2/partB/the_trainer.py", line 94, in train
    model.to(device)
  File "/home/speech/.conda/envs/dl_things/lib/python3.10/site-packages/torch/nn/modules/module.py", line 927, in to
    return self._apply(convert)
  File "/home/speech/.conda/envs/dl_things/lib/python3.10/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/speech/.conda/envs/dl_things/lib/python3.10/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/speech/.conda/envs/dl_things/lib/python3.10/site-packages/torch/nn/modules/module.py", line 925, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.65 GiB total capacity; 1.18 GiB already allocated; 3.12 MiB free; 1.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
